ssh://hamed.babaei@192.168.33.29:22/home/hamed.babaei/.conda/envs/hb/bin/python3.9 -u /home/hamed.babaei/projects/hatespeech/bert_runner.py
transformers version: 4.12.5
TRAINING TFIDF + ML MODEL.....
Size of the train set is: 25569, Number of hate speech samples:1786
Size of the test set is: 6393, Number of hate speech samples:456
STARTING THE FINE TUNING ROBERTA FOR DOWNSTREAM TASK
Some weights of the model checkpoint at /mnt/disk2/transformers/roberta were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /mnt/disk2/transformers/roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|                                                     | 0/1 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
100%|█████████████████████████████████████████████| 1/1 [00:30<00:00, 30.76s/ba]
100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.62ba/s]
The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, tweets.
***** Running training *****
  Num examples = 25569
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 24
  Gradient Accumulation steps = 1
  Total optimization steps = 3198
{'loss': 0.1606, 'learning_rate': 4.218261413383365e-05, 'epoch': 0.47}
 16%|██████▎                                 | 500/3198 [02:47<14:44,  3.05it/s]Saving model checkpoint to /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-500
Configuration saved in /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-500/config.json
Model weights saved in /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-500/pytorch_model.bin
Deleting older checkpoint [/mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-1000] due to args.save_total_limit
{'loss': 0.1048, 'learning_rate': 3.4365228267667294e-05, 'epoch': 0.94}
 31%|████████████▏                          | 1000/3198 [05:33<12:00,  3.05it/s]Saving model checkpoint to /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-1000
Configuration saved in /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-1000/config.json
Model weights saved in /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-1000/pytorch_model.bin
Deleting older checkpoint [/mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-500] due to args.save_total_limit
 33%|█████████████                          | 1066/3198 [05:57<10:24,  3.41it/s]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, tweets.
***** Running Evaluation *****
  Num examples = 6393
  Batch size = 24

 99%|████████████████████████████████████████▌| 264/267 [00:26<00:00, 10.06it/s]

{'eval_loss': 0.07536273449659348, 'eval_accuracy': 0.9773189425934616, 'eval_f1': 0.907107214739628, 'eval_precision': 0.9459166518229671, 'eval_recall': 0.8754260746611349, 'eval_runtime': 26.5747, 'eval_samples_per_second': 240.567, 'eval_steps_per_second': 10.047, 'epoch': 1.0}
 33%|█████████████                          | 1066/3198 [06:24<10:24,  3.41it/s]
100%|█████████████████████████████████████████| 267/267 [00:26<00:00, 10.06it/s]
{'loss': 0.0661, 'learning_rate': 2.6547842401500937e-05, 'epoch': 1.41}
 47%|██████████████████▎                    | 1500/3198 [08:47<09:14,  3.06it/s]Saving model checkpoint to /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-1500
Configuration saved in /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-1500/config.json
Model weights saved in /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-1500/pytorch_model.bin
Deleting older checkpoint [/mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-1000] due to args.save_total_limit
{'loss': 0.0632, 'learning_rate': 1.8730456535334583e-05, 'epoch': 1.88}
 63%|████████████████████████▍              | 2000/3198 [11:34<06:39,  3.00it/s]Saving model checkpoint to /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-2000
Configuration saved in /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-2000/config.json
Model weights saved in /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-2000/pytorch_model.bin
Deleting older checkpoint [/mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-1500] due to args.save_total_limit
 67%|██████████████████████████             | 2132/3198 [12:20<05:11,  3.42it/s]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, tweets.
***** Running Evaluation *****
  Num examples = 6393
  Batch size = 24

 99%|████████████████████████████████████████▌| 264/267 [00:25<00:00, 10.04it/s]

{'eval_loss': 0.06128648295998573, 'eval_accuracy': 0.9812294697325199, 'eval_f1': 0.928578663643825, 'eval_precision': 0.9321173254835997, 'eval_recall': 0.9251083747772666, 'eval_runtime': 26.1401, 'eval_samples_per_second': 244.567, 'eval_steps_per_second': 10.214, 'epoch': 2.0}
 67%|██████████████████████████             | 2132/3198 [12:46<05:11,  3.42it/s]
100%|█████████████████████████████████████████| 267/267 [00:26<00:00, 10.04it/s]
{'loss': 0.0433, 'learning_rate': 1.0913070669168231e-05, 'epoch': 2.35}
 78%|██████████████████████████████▍        | 2500/3198 [14:47<03:47,  3.06it/s]Saving model checkpoint to /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-2500
Configuration saved in /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-2500/config.json
Model weights saved in /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-2500/pytorch_model.bin
Deleting older checkpoint [/mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-2000] due to args.save_total_limit
{'loss': 0.0332, 'learning_rate': 3.095684803001876e-06, 'epoch': 2.81}
 94%|████████████████████████████████████▌  | 3000/3198 [17:33<01:04,  3.06it/s]Saving model checkpoint to /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-3000
Configuration saved in /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-3000/config.json
Model weights saved in /mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-3000/pytorch_model.bin
Deleting older checkpoint [/mnt/disk2/hbmodels/hatespeech-roberta/checkpoint-2500] due to args.save_total_limit
100%|███████████████████████████████████████| 3198/3198 [18:40<00:00,  3.43it/s]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, tweets.
***** Running Evaluation *****
  Num examples = 6393
  Batch size = 24

 99%|████████████████████████████████████████▌| 264/267 [00:25<00:00, 10.15it/s]

{'eval_loss': 0.0748298168182373, 'eval_accuracy': 0.9842014703582043, 'eval_f1': 0.937914427133949, 'eval_precision': 0.9569729758361984, 'eval_recall': 0.9206348678669893, 'eval_runtime': 25.5369, 'eval_samples_per_second': 250.344, 'eval_steps_per_second': 10.455, 'epoch': 3.0}
100%|███████████████████████████████████████| 3198/3198 [19:05<00:00,  3.43it/s]
100%|█████████████████████████████████████████| 267/267 [00:25<00:00, 10.28it/s]


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 1145.8953, 'train_samples_per_second': 66.941, 'train_steps_per_second': 2.791, 'train_loss': 0.07493602983499781, 'epoch': 3.0}
100%|███████████████████████████████████████| 3198/3198 [19:05<00:00,  2.79it/s]
Saving model checkpoint to /mnt/disk2/hbmodels/hatespeech-roberta
Configuration saved in /mnt/disk2/hbmodels/hatespeech-roberta/config.json
Model weights saved in /mnt/disk2/hbmodels/hatespeech-roberta/pytorch_model.bin
The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, tweets.
***** Running Evaluation *****
  Num examples = 6393
  Batch size = 24
100%|█████████████████████████████████████████| 267/267 [00:26<00:00,  9.96it/s]
{'epoch': 3.0,
 'eval_accuracy': 0.9842014703582043,
 'eval_f1': 0.937914427133949,
 'eval_loss': 0.0748298168182373,
 'eval_precision': 0.9569729758361984,
 'eval_recall': 0.9206348678669893,
 'eval_runtime': 26.9136,
 'eval_samples_per_second': 237.538,
 'eval_steps_per_second': 9.921}
The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, tweets.
***** Running Prediction *****
  Num examples = 6393
  Batch size = 24
100%|████████████████████████████████████████▊| 266/267 [00:26<00:00, 10.07it/s]{'F1 Macro': 0.937914427133949,
 'accuracy': 0.9842014703582043,
 'classification-report': '              precision    recall  f1-score   '
                          'support\n'
                          '\n'
                          '           0       0.99      0.99      0.99      '
                          '5937\n'
                          '           1       0.93      0.85      0.88       '
                          '456\n'
                          '\n'
                          '    accuracy                           0.98      '
                          '6393\n'
                          '   macro avg       0.96      0.92      0.94      '
                          '6393\n'
                          'weighted avg       0.98      0.98      0.98      '
                          '6393\n'}
Save results with gt and predicts into :/home/hamed.babaei/projects/hatespeech/logs/twitter-evaluation-roberta.json
ENF OF THE FINE TUNING!
100%|█████████████████████████████████████████| 267/267 [00:26<00:00,  9.92it/s]

Process finished with exit code 0
